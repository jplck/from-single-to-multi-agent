{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import (\n",
    "    MagenticOneGroupChat,\n",
    ")\n",
    "from autogen_agentchat.teams._group_chat._magentic_one._magentic_one_orchestrator import MagenticOneOrchestrator\n",
    "from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing required field 'json_output' in ModelInfo. Starting in v0.4.7, the required fields are enforced.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m api_key = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mAZURE_OPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model_client = \u001b[43mAzureOpenAIChatCompletionClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mazure_deployment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAZURE_OPENAI_COMPLETION_MODEL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAZURE_OPENAI_API_VERSION\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mazure_endpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAZURE_OPENAI_ENDPOINT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_calling\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvision\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfamily\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py:1431\u001b[39m, in \u001b[36mAzureOpenAIChatCompletionClient.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1429\u001b[39m create_args = _create_args_from_config(copied_args)\n\u001b[32m   1430\u001b[39m \u001b[38;5;28mself\u001b[39m._raw_config: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = copied_args\n\u001b[32m-> \u001b[39m\u001b[32m1431\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_capabilities\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_capabilities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1436\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_name_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_name_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1437\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py:379\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient.__init__\u001b[39m\u001b[34m(self, client, create_args, model_capabilities, model_info, add_name_prefixes)\u001b[39m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28mself\u001b[39m._model_info = model_info\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Validate model_info, check if all required fields are present\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m \u001b[43mvalidate_model_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[38;5;28mself\u001b[39m._resolved_model: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m create_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_core/models/_model_client.py:121\u001b[39m, in \u001b[36mvalidate_model_info\u001b[39m\u001b[34m(model_info)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m required_fields:\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m field \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_info:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    122\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required field \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m in ModelInfo. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mStarting in v0.4.7, the required fields are enforced.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Missing required field 'json_output' in ModelInfo. Starting in v0.4.7, the required fields are enforced."
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\"),\n",
    "    model=os.getenv(\"AZURE_OPENAI_COMPLETION_MODEL\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=api_key,\n",
    "    model_info={\n",
    "        \"json_output\": True,\n",
    "        \"function_calling\": True,\n",
    "        \"vision\": False,\n",
    "        \"family\": \"unknown\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_weather(city: str) -> str:\n",
    "    print(\"executing get_weather\")\n",
    "    return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
    "\n",
    "async def get_medical_history(username: str) -> str:\n",
    "    \"Get the medical history for a given username with known allergies and food restrictions.\"\n",
    "    print(\"executing get_medical_history\")\n",
    "    return f\"{username} has an allergy to peanuts and eggs.\"\n",
    "\n",
    "async def get_available_incredients(location: str) -> str:\n",
    "    \"Get the available incredients for a given location.\"\n",
    "    print(\"executing get_available_incredients\")\n",
    "    return f\"Available incredients in {location} are: eggs, milk, bread, peanuts, beer, wine, salmon, spinache, oil and butter.\"\n",
    "\n",
    "def get_current_username(input: str) -> str:\n",
    "    \"Get the username of the current user.\"\n",
    "    print(\"executing get_current_username\")\n",
    "    return \"Dennis\"\n",
    "\n",
    "def get_current_location_of_user(username: str) -> str:\n",
    "    \"Get the current timezone location of the user for a given username.\"\n",
    "    print(\"executing get_current_location\")\n",
    "    print(username)\n",
    "    if \"Dennis\" in username:\n",
    "        return \"Europe/Berlin\"\n",
    "    else:\n",
    "        return \"America/New_York\"\n",
    "\n",
    "def get_current_time(location: str) -> str:\n",
    "    \"Get the current time in the given location. The pytz is used to get the timezone for that location. Location names should be in a format like America/Seattle, Asia/Bangkok, Europe/London. Anything in Germany should be Europe/Berlin\"\n",
    "    try:\n",
    "        print(\"get current time for location: \", location)\n",
    "        timezone = pytz.timezone(location)\n",
    "        # Get the current time in the timezone\n",
    "        now = datetime.now(timezone)\n",
    "        current_time = now.strftime(\"%I:%M:%S %p\")\n",
    "        return current_time\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "        return \"Sorry, I couldn't find the timezone for that location.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_agent = AssistantAgent(\n",
    "    \"users_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_current_username, get_medical_history],\n",
    "    description=\"A helpful assistant that can knows things about the user like the username and the medical history of the user.\",\n",
    "    system_message=\"You are a helpful assistant that can retrieve the username and medical history of the current user.\",\n",
    ")\n",
    "\n",
    "location_agent = AssistantAgent(\n",
    "    \"location_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_current_location_of_user],\n",
    "    description=\"A assistant that can find the physical location of a user.\",\n",
    "    system_message=\"You are a helpful assistant that can suggest details for a location and can utilize any context information provided.\",\n",
    ")\n",
    "\n",
    "time_agent = AssistantAgent(\n",
    "    \"time_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_current_time],\n",
    "    description=\"A helpful assistant that knows time in a specific location.\",\n",
    "    system_message=\"You are a helpful assistant that can retrieve the current time for a given location.\",\n",
    ")\n",
    "\n",
    "chef_agent = AssistantAgent(\n",
    "    \"chef_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_available_incredients],\n",
    "    description=\"A helpful assistant that can suggest meals and dishes for the right time of the day, location, available ingredients, user preferences and allergies.\",\n",
    "    system_message=\"You are a helpful assistant that can recommend dishes for the right time of the day, location, available ingredients and user preferences. Make sure you ask for individual food preferences and allergies as input. If you do not have concrete information about allergies you must ask the question and not prepare a dish until you get information.\",\n",
    ")\n",
    "\n",
    "\n",
    "summary_agent = AssistantAgent(\n",
    "    \"summary_agent\",\n",
    "    model_client=model_client,\n",
    "    description=\"A helpful assistant that can summarize details about conversations.\",\n",
    "    system_message=\"You are a helpful assistant that can take in all of the suggestions and advice from the other agents and leverage them to answer questions. You must ensure that you use that the other agents can solve the problem. When all open questions have been answered, you can respond with TERMINATE.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <coroutine object main at 0x7f2a21c1c310>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n",
      "Exception ignored in: <coroutine object main at 0x7f2a21c1c310>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyError: '__import__'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model does not support JSON output.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     stream = magenticteam.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33mI want to have something to eat. What would you recommend?.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m Console(stream)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Run the team and stream messages to the console\u001b[39;00m\n\u001b[32m      7\u001b[39m stream = magenticteam.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33mI want to have something to eat. What would you recommend?.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:445\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token)\u001b[39m\n\u001b[32m    439\u001b[39m     shutdown_task = asyncio.create_task(stop_runtime())\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# Run the team by sending the start message to the group chat manager.\u001b[39;00m\n\u001b[32m    443\u001b[39m     \u001b[38;5;66;03m# The group chat manager will start the group chat by relaying the message to the participants\u001b[39;00m\n\u001b[32m    444\u001b[39m     \u001b[38;5;66;03m# and the group chat manager.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._runtime.send_message(\n\u001b[32m    446\u001b[39m         GroupChatStart(messages=messages),\n\u001b[32m    447\u001b[39m         recipient=AgentId(\u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mself\u001b[39m._group_chat_manager_topic_type, key=\u001b[38;5;28mself\u001b[39m._team_id),\n\u001b[32m    448\u001b[39m         cancellation_token=cancellation_token,\n\u001b[32m    449\u001b[39m     )\n\u001b[32m    450\u001b[39m     \u001b[38;5;66;03m# Collect the output messages in order.\u001b[39;00m\n\u001b[32m    451\u001b[39m     output_messages: List[AgentEvent | ChatMessage] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:332\u001b[39m, in \u001b[36mSingleThreadedAgentRuntime.send_message\u001b[39m\u001b[34m(self, message, recipient, sender, cancellation_token, message_id)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._message_queue.put(\n\u001b[32m    319\u001b[39m     SendMessageEnvelope(\n\u001b[32m    320\u001b[39m         message=message,\n\u001b[32m   (...)\u001b[39m\u001b[32m    327\u001b[39m     )\n\u001b[32m    328\u001b[39m )\n\u001b[32m    330\u001b[39m cancellation_token.link_future(future)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:422\u001b[39m, in \u001b[36mSingleThreadedAgentRuntime._process_send\u001b[39m\u001b[34m(self, message_envelope)\u001b[39m\n\u001b[32m    420\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tracer_helper.trace_block(\u001b[33m\"\u001b[39m\u001b[33mprocess\u001b[39m\u001b[33m\"\u001b[39m, recipient_agent.id, parent=message_envelope.metadata):\n\u001b[32m    421\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m MessageHandlerContext.populate_context(recipient_agent.id):\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m             response = \u001b[38;5;28;01mawait\u001b[39;00m recipient_agent.on_message(\n\u001b[32m    423\u001b[39m                 message_envelope.message,\n\u001b[32m    424\u001b[39m                 ctx=message_context,\n\u001b[32m    425\u001b[39m             )\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m CancelledError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message_envelope.future.cancelled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_core/_base_agent.py:113\u001b[39m, in \u001b[36mBaseAgent.on_message\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_message\u001b[39m(\u001b[38;5;28mself\u001b[39m, message: Any, ctx: MessageContext) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_message_impl(message, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py:67\u001b[39m, in \u001b[36mSequentialRoutedAgent.on_message_impl\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fifo_lock.acquire()\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().on_message_impl(message, ctx)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Release the FIFO lock to allow the next message to be processed.\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mself\u001b[39m._fifo_lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_core/_routed_agent.py:485\u001b[39m, in \u001b[36mRoutedAgent.on_message_impl\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    484\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m h.router(message, ctx):\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m h(\u001b[38;5;28mself\u001b[39m, message, ctx)\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_unhandled_message(message, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_core/_routed_agent.py:389\u001b[39m, in \u001b[36mrpc.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    387\u001b[39m         logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMessage type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in target types \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m return_value = \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, message, ctx)\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m AnyType \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m return_types \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(return_value) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m return_types:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_magentic_one/_magentic_one_orchestrator.py:180\u001b[39m, in \u001b[36mMagenticOneOrchestrator.handle_start\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Kick things off\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;28mself\u001b[39m._n_stalls = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._reenter_outer_loop(ctx.cancellation_token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_magentic_one/_magentic_one_orchestrator.py:277\u001b[39m, in \u001b[36mMagenticOneOrchestrator._reenter_outer_loop\u001b[39m\u001b[34m(self, cancellation_token)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.publish_message(\n\u001b[32m    272\u001b[39m     GroupChatAgentResponse(agent_response=Response(chat_message=ledger_message)),\n\u001b[32m    273\u001b[39m     topic_id=DefaultTopicId(\u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mself\u001b[39m._group_topic_type),\n\u001b[32m    274\u001b[39m )\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Restart the inner loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._orchestrate_step(cancellation_token=cancellation_token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_magentic_one/_magentic_one_orchestrator.py:298\u001b[39m, in \u001b[36mMagenticOneOrchestrator._orchestrate_step\u001b[39m\u001b[34m(self, cancellation_token)\u001b[39m\n\u001b[32m    296\u001b[39m key_error: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._max_json_retries):\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_client.create(\u001b[38;5;28mself\u001b[39m._get_compatible_context(context), json_output=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    299\u001b[39m     ledger_str = response.content\n\u001b[32m    300\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py:449\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient.create\u001b[39m\u001b[34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m json_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    448\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_info[\u001b[33m\"\u001b[39m\u001b[33mjson_output\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m json_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mModel does not support JSON output.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m json_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    452\u001b[39m         create_args[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mjson_object\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[31mValueError\u001b[39m: Model does not support JSON output."
     ]
    }
   ],
   "source": [
    "async def main() -> None:\n",
    "\n",
    "    inner_termination = MaxMessageTermination(20)\n",
    "    magenticteam = MagenticOneGroupChat([users_agent, location_agent, time_agent, chef_agent], model_client=model_client, termination_condition=inner_termination)\n",
    "\n",
    "    # Run the team and stream messages to the console\n",
    "    stream = magenticteam.run_stream(task=\"I want to have something to eat. What would you recommend?.\")\n",
    "    await Console(stream)\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
