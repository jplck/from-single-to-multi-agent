{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60017efc",
   "metadata": {},
   "source": [
    "# Building Multi-Agent Systems with AutoGen\n",
    "\n",
    "In this notebook, we'll explore how to build intelligent agents using the AutoGen framework. We'll start with the basics and progress to building a multi-agent system.\n",
    "\n",
    "## What is AutoGen?\n",
    "\n",
    "AutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents can:\n",
    "- Process and respond to user queries\n",
    "- Use tools (functions) to interact with external systems\n",
    "- Maintain context through conversation\n",
    "- Be orchestrated to work together on complex tasks\n",
    "\n",
    "Let's start by setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install autogen-agentchat autogen-ext python-dotenv pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafce536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import asyncio\n",
    "import os\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import MagenticOneGroupChat\n",
    "from autogen_agentchat.teams._group_chat._magentic_one._magentic_one_orchestrator import MagenticOneOrchestrator\n",
    "from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient, AzureOpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5dc3b",
   "metadata": {},
   "source": [
    "## 1. Setting Up Azure OpenAI\n",
    "\n",
    "To use agents with AutoGen, we need to set up a connection to an LLM service. In this example, we'll use GitHub Models, which gives us easy access to the latest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ea417",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"GITHUB_TOKEN\") is None:\n",
    "    model_client = AzureOpenAIChatCompletionClient(\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\"),\n",
    "        model=os.getenv(\"AZURE_OPENAI_COMPLETION_MODEL\"),\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "        model_info={\n",
    "            \"json_output\": True,\n",
    "            \"function_calling\": True,\n",
    "            \"vision\": False,\n",
    "            \"family\": \"unknown\",\n",
    "        },\n",
    "    )\n",
    "else:\n",
    "    # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings. \n",
    "    # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\n",
    "    token = os.environ[\"GITHUB_TOKEN\"]\n",
    "    endpoint = \"https://models.inference.ai.azure.com\"\n",
    "    model_name = \"gpt-4o-mini\"\n",
    "\n",
    "    # Create a model client for AutoGen\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=model_name,\n",
    "        base_url=endpoint,\n",
    "        api_key=token\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9948a6e",
   "metadata": {},
   "source": [
    "## 2. Creating Tools\n",
    "\n",
    "In AutoGen, tools are functions that agents can use to perform tasks. Let's define several tools that our agents can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools for our agents\n",
    "async def get_weather(city: str) -> str:\n",
    "    \"\"\"Gets a statement about the current weathr in the city defined in the parameter\"\"\"\n",
    "    print(\"executing get_weather\")\n",
    "    return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
    "\n",
    "async def get_medical_history(username: str) -> str:\n",
    "    \"Get the medical history for a given username with known allergies and food restrictions.\"\n",
    "    print(\"executing get_medical_history\")\n",
    "    return f\"{username} has an allergy to peanuts and eggs.\"\n",
    "\n",
    "async def get_available_incredients(location: str) -> str:\n",
    "    \"Get the available incredients for a given location.\"\n",
    "    print(\"executing get_available_incredients\")\n",
    "    return f\"Available incredients in {location} are: eggs, milk, bread, peanuts, beer, wine, salmon, spinache, oil and butter.\"\n",
    "\n",
    "def get_current_username(input: str) -> str:\n",
    "    \"Get the username of the current user.\"\n",
    "    print(\"executing get_current_username\")\n",
    "    return \"Dennis\"\n",
    "\n",
    "def get_current_location_of_user(username: str) -> str:\n",
    "    \"Get the current timezone location of the user for a given username.\"\n",
    "    print(\"executing get_current_location\")\n",
    "    print(username)\n",
    "    if \"Dennis\" in username:\n",
    "        return \"Europe/Berlin\"\n",
    "    else:\n",
    "        return \"America/New_York\"\n",
    "\n",
    "def get_current_time(location: str) -> str:\n",
    "    \"Get the current time in the given location. The pytz is used to get the timezone for that location. Location names should be in a format like America/Seattle, Asia/Bangkok, Europe/London. Anything in Germany should be Europe/Berlin\"\n",
    "    try:\n",
    "        print(\"get current time for location: \", location)\n",
    "        timezone = pytz.timezone(location)\n",
    "        # Get the current time in the timezone\n",
    "        now = datetime.now(timezone)\n",
    "        current_time = now.strftime(\"%I:%M:%S %p\")\n",
    "        return current_time\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "        return \"Sorry, I couldn't find the timezone for that location.\"\n",
    "    \n",
    "\n",
    "# Test one of our tools\n",
    "await get_weather(\"Seattle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1373e5",
   "metadata": {},
   "source": [
    "## 3. Creating Specialized Agents\n",
    "\n",
    "Now let's create specialized agents, each with their own set of tools and responsibilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ef054",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_agent = AssistantAgent(\n",
    "    \"users_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_current_username, get_medical_history],\n",
    "    description=\"A helpful assistant that can knows things about the user like the username and the medical history of the user.\",\n",
    "    system_message=\"You are a helpful assistant that can retrieve the username and medical history of the current user.\",\n",
    ")\n",
    "\n",
    "location_agent = AssistantAgent(\n",
    "    \"location_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_current_location_of_user],\n",
    "    description=\"A assistant that can find the physical location of a user.\",\n",
    "    system_message=\"You are a helpful assistant that can suggest details for a location and can utilize any context information provided.\",\n",
    ")\n",
    "\n",
    "time_agent = AssistantAgent(\n",
    "    \"time_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_current_time],\n",
    "    description=\"A helpful assistant that knows time in a specific location.\",\n",
    "    system_message=\"You are a helpful assistant that can retrieve the current time for a given location.\",\n",
    ")\n",
    "\n",
    "chef_agent = AssistantAgent(\n",
    "    \"chef_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_available_incredients],\n",
    "    description=\"A helpful assistant that can suggest meals and dishes for the right time of the day, location, available ingredients, user preferences and allergies.\",\n",
    "    system_message=\"You are a helpful assistant that can recommend dishes for the right time of the day, location, available ingredients and user preferences. Make sure you ask for individual food preferences and allergies as input. If you do not have concrete information about allergies you must ask the question and not prepare a dish until you get information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e6fc2",
   "metadata": {},
   "source": [
    "## 4. Creating a Summary Agent\n",
    "\n",
    "In addition to our specialized agents, we'll create a summary agent that can synthesize the information from all other agents and provide a final response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e2b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary agent\n",
    "summary_agent = AssistantAgent(\n",
    "    \"summary_agent\",\n",
    "    model_client=model_client,\n",
    "    description=\"A helpful assistant that can summarize details about conversations.\",\n",
    "    system_message=\"You are a helpful assistant that can take in all of the suggestions and advice from the other agents and leverage them to answer questions. You must ensure that you use the other agents. If you are done you respond with TERMINATE.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679a7bc",
   "metadata": {},
   "source": [
    "## 5. Running a Multi-Agent Conversation\n",
    "\n",
    "Now that we have created our agents, let's put them to work together in a multi-agent conversation. In AutoGen, we use group chat implementations to orchestrate the conversation between agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb4dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_multi_agent_conversation():\n",
    "    # Set up termination condition - limit to 10 messages to avoid infinite loops\n",
    "    inner_termination = MaxMessageTermination(10)\n",
    "    # Set up termination condition - look for the word \"TERMINATE\" in agent messages\n",
    "    termination_condition = TextMentionTermination(\"TERMINATE\")\n",
    "\n",
    "    combined_termination = inner_termination | termination_condition\n",
    "    \n",
    "    # Create a Magentic One group chat with our specialized agents\n",
    "    # This is a type of group chat where agents can collaborate based on their specialized knowledge\n",
    "    magenticteam = MagenticOneGroupChat(\n",
    "        [users_agent, location_agent, time_agent, chef_agent, summary_agent], \n",
    "        model_client=model_client, \n",
    "        termination_condition=combined_termination,\n",
    "        final_answer_prompt=\"Don't ask the user for anything else. Just provide the final answer.\"\n",
    "    )\n",
    "\n",
    "    # Run the team and stream messages to the console\n",
    "    stream = magenticteam.run_stream(task=\"I want to have something to eat. What would you recommend?\")\n",
    "    await Console(stream)\n",
    "\n",
    "# Run our multi-agent conversation\n",
    "await run_multi_agent_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c56553",
   "metadata": {},
   "source": [
    "## 6. Understanding MagenticOneGroupChat\n",
    "\n",
    "The `MagenticOneGroupChat` is a specific type of group chat in AutoGen that uses a special orchestration method to determine which agent should respond next. This is different from the `RoundRobinGroupChat`, which would cycle through the agents in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using a round robin chat (alternative to MagenticOne)\n",
    "async def run_round_robin_chat():\n",
    "    # Set up termination condition - limit to 10 messages to avoid infinite loops\n",
    "    inner_termination = MaxMessageTermination(10)\n",
    "    # Set up termination condition - look for the word \"TERMINATE\" in agent messages\n",
    "    termination_condition = TextMentionTermination(\"TERMINATE\")\n",
    "    \n",
    "    # Create a round robin chat where agents take turns in order\n",
    "    # We're adding the summary agent to help bring the conversation to a conclusion\n",
    "    round_robin_team = RoundRobinGroupChat(\n",
    "        [users_agent, location_agent, time_agent, chef_agent, summary_agent],\n",
    "        termination_condition=termination_condition\n",
    "    )\n",
    "    \n",
    "    # Run the team with a different task\n",
    "    stream = round_robin_team.run_stream(task=\"I'm hungry but I need to consider my dietary restrictions. What should I eat?\")\n",
    "    await Console(stream)\n",
    "\n",
    "await run_round_robin_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4932c92",
   "metadata": {},
   "source": [
    "## 7. Exercise: Create Your Own Multi-Agent System\n",
    "\n",
    "Now it's your turn! Try creating your own multi-agent system with AutoGen. Here are some ideas:\n",
    "- A travel planning system with agents for flights, hotels, activities, and weather\n",
    "- A research assistant system with agents for searching, summarizing, and fact-checking\n",
    "- A customer support system with agents for different types of issues\n",
    "\n",
    "Here's a template to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217904d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own tools\n",
    "async def my_custom_tool(parameter: str) -> str:\n",
    "    \"\"\"Description of what your tool does\"\"\"\n",
    "    print(f\"Executing my_custom_tool with {parameter}\")\n",
    "    # Your tool implementation here\n",
    "    return f\"Result for {parameter}\"\n",
    "\n",
    "# Create your own agents\n",
    "my_agent = AssistantAgent(\n",
    "    \"my_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[my_custom_tool],\n",
    "    description=\"Description of what your agent does\",\n",
    "    system_message=\"Detailed instructions for your agent\",\n",
    ")\n",
    "\n",
    "# Create a group chat with your agents\n",
    "# my_team = MagenticOneGroupChat([my_agent, ...], model_client=model_client)\n",
    "\n",
    "# Run your team\n",
    "# stream = my_team.run_stream(task=\"Your task here\")\n",
    "# await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57b86aa",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've explored how to build intelligent multi-agent systems using AutoGen:\n",
    "\n",
    "1. We set up a connection to Azure OpenAI (GitHub Models) for the LLM backend.\n",
    "2. We created tools that agents can use to interact with external systems.\n",
    "3. We built specialized agents with clearly defined roles and tools.\n",
    "4. We orchestrated multiple agents to collaboratively solve a task using group chats.\n",
    "\n",
    "AutoGen provides a powerful framework for creating advanced multi-agent systems that can solve complex tasks through the collaboration of specialized agents. This approach allows for more robust and capable AI systems that can leverage the strengths of different types of agents working together.\n",
    "\n",
    "For more information, visit the [AutoGen documentation](https://microsoft.github.io/autogen/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
