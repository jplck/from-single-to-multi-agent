{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c188c18f",
   "metadata": {},
   "source": [
    "# Building a Group Chat with AutoGen\n",
    "\n",
    "In this notebook, we create a multi-agent group chat using the autogen library.\n",
    "\n",
    "The logical stack for combining multiple agents in a platform requries the following logical stack\n",
    "\n",
    "![magentic](../../assets/images/logicalstack.png)\n",
    "\n",
    "This playground is about demonstrating the pattern of dynamic unplanned collaboration between different agent types to solve abstract problems\n",
    "![magentic](../../assets/images/magentic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af8c43",
   "metadata": {},
   "source": [
    "## 1. Setting Up Our Environment\n",
    "\n",
    "We will import the necessary libraries and configure our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import MagenticOneGroupChat\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient, AzureOpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802920c2",
   "metadata": {},
   "source": [
    "## 2. Configuring the Model Client\n",
    "\n",
    "Set up the model client based on available environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"GITHUB_TOKEN\") is None:\n",
    "    model_client = AzureOpenAIChatCompletionClient(\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\"),\n",
    "        model=os.getenv(\"AZURE_OPENAI_COMPLETION_MODEL\"),\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "        model_info={\n",
    "            \"json_output\": True,\n",
    "            \"function_calling\": True,\n",
    "            \"vision\": False,\n",
    "            \"family\": \"unknown\",\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings. \n",
    "    # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\n",
    "    token = os.environ[\"GITHUB_TOKEN\"]\n",
    "    endpoint = \"https://models.inference.ai.azure.com\"\n",
    "    model_name = \"gpt-4o-mini\"\n",
    "\n",
    "    # Create a model client for AutoGen\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=model_name,\n",
    "        base_url=endpoint,\n",
    "        api_key=token\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dcce49",
   "metadata": {},
   "source": [
    "## 3. Defining Tools and Agents\n",
    "\n",
    "We define tools and specialized agents to handle specific tasks such as retrieving usernames, locations, and current time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "async def get_weather(city: str) -> str:\n",
    "    return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
    "\n",
    "def get_current_username(input: str) -> str:\n",
    "    \"Get the username of the current user.\"\n",
    "    print(\"executing get_current_username\")\n",
    "    return \"Dennis\"\n",
    "\n",
    "def get_current_location_of_user(username: str) -> str:\n",
    "    \"Get the current timezone location of the user for a given username.\"\n",
    "    print(\"executing get_current_location\")\n",
    "    print(username)\n",
    "    if \"Dennis\" in username:\n",
    "        return \"Europe/Berlin\"\n",
    "    else:\n",
    "        return \"America/New_York\"\n",
    "\n",
    "def get_current_time(location: str) -> str:\n",
    "    \"Get the current time in the given location. The pytz is used to get the timezone for that location. Location names should be in a format like America/Seattle, Asia/Bangkok, Europe/London. Anything in Germany should be Europe/Berlin\"\n",
    "    try:\n",
    "        print(\"get current time for location: \", location)\n",
    "        timezone = pytz.timezone(location)\n",
    "        # Get the current time in the timezone\n",
    "        now = datetime.now(timezone)\n",
    "        current_time = now.strftime(\"%I:%M:%S %p\")\n",
    "        return current_time\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "        return \"Sorry, I couldn't find the timezone for that location.\"\n",
    "    \n",
    "\n",
    "# Define agents\n",
    "users_agent = AssistantAgent(\n",
    "    \"users_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_current_username],\n",
    "    description=\"A helpful assistant that can knows things about the user like the username.\",\n",
    "    system_message=\"You are a helpful assistant that can retrieve the username of the current user.\",\n",
    ")\n",
    "\n",
    "location_agent = AssistantAgent(\n",
    "    \"location_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_current_location_of_user],\n",
    "    description=\"A assistant that can find the physical location of a user.\",\n",
    "    system_message=\"You are a helpful assistant that can suggest details for a location and can utilize any context information provided.\",\n",
    ")\n",
    "\n",
    "time_agent = AssistantAgent(\n",
    "    \"time_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_current_time],\n",
    "    description=\"A helpful assistant that knows time in a specific location.\",\n",
    "    system_message=\"You are a helpful assistant that can retrieve the current time for a given location.\",\n",
    ")\n",
    "\n",
    "summary_agent = AssistantAgent(\n",
    "    \"summary_agent\",\n",
    "    model_client=model_client,\n",
    "    description=\"A helpful assistant that can summarize details about conversations.\",\n",
    "    system_message=\"You are a helpful assistant that can take in all of the suggestions and advice from the other agents and leverage them to answer questions. You must ensure that you use that the other agents can solve the problem. When all open questions have been answered, you can respond with TERMINATE.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb24ac",
   "metadata": {},
   "source": [
    "## 4. Running the Group Chat\n",
    "\n",
    "We now create an interactive function to run the group chat with different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_group_chat(query: str) -> None:\n",
    "    \"\"\"Run the group chat with a specific query\"\"\"\n",
    "    # Set up termination conditions\n",
    "    inner_termination = MaxMessageTermination(5)\n",
    "    \n",
    "    # Create the group chat\n",
    "    magenticteam = MagenticOneGroupChat(\n",
    "        [users_agent, location_agent, time_agent],\n",
    "        model_client=model_client,\n",
    "        termination_condition=inner_termination\n",
    "    )\n",
    "    \n",
    "    # Run the team and stream messages to the console\n",
    "    stream = magenticteam.run_stream(task=query)\n",
    "    await Console(stream)\n",
    "\n",
    "# Example query to run\n",
    "await run_group_chat(\"what time is it here?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ed2e4",
   "metadata": {},
   "source": [
    "## 5. Interactive Group Chat\n",
    "\n",
    "Now let's try an interactive version where you can input your own queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bdac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def interactive_chat():\n",
    "    while True:\n",
    "        query = input(\"Enter your query (or 'exit' to quit): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        await run_group_chat(query)\n",
    "        print(\"\\n---Chat complete---\\n\")\n",
    "\n",
    "# Run the interactive chat\n",
    "await interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed37ca",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to build a multi-agent group chat using autogen. You can create additional agents or refine the conversation flow to suit various collaborative tasks.\n",
    "\n",
    "Try experimenting with different types of questions like:\n",
    "- \"What time is it in New York?\"\n",
    "- \"Who am I?\"\n",
    "- \"What's my location?\"\n",
    "\n",
    "Each of these questions will engage different agents in the group chat based on their specializations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
