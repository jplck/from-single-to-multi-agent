{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics - Setting everything up\n",
    "\n",
    "Open this repository in a GitHub Codespace.\n",
    "Before you start with anything else, make sure you setup the infrastructure required. Follow the readme file in the root folder to do this!\n",
    "\n",
    "To start with the workshop, if not already done, or opened in a Devcontainer run this in the top level folder:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the Azure OpenAI API library\n",
    "\n",
    "In this notebook, we will explore how to use OpenAI's language models (LLMs) to generate responses to various types of questions. The code in the next cell sets up the necessary environment by importing required modules, loading environment variables, and initializing the Azure OpenAI client. This setup is crucial for making API calls to OpenAI's services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "    \n",
    "# To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings. \n",
    "# Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\n",
    "\n",
    "model = os.getenv(\"AZURE_OPENAI_COMPLETION_MODEL\") if os.getenv(\"AZURE_OPENAI_COMPLETION_MODEL\") is not None else \"gpt-4o-mini\"\n",
    "\n",
    "if os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n",
    "    print(\"Using Azure OpenAI\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    )\n",
    "else:\n",
    "    print(\"Using Inference API\")\n",
    "    base_url=\"https://models.inference.ai.azure.com\"\n",
    "    api_key=os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=base_url,\n",
    "        api_key=api_key,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if stuff works in general, you can run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = model,    \n",
    "    messages = [{\"role\" : \"assistant\", \"content\" : \"The one thing I love more than anything else is \"}],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the object model for receiving questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class QuestionType(str, Enum):\n",
    "    multiple_choice = \"multiple_choice\"\n",
    "    true_or_false = \"true_or_false\"\n",
    "    estimation = \"estimation\"\n",
    "\n",
    "class Ask(BaseModel):\n",
    "    question: str | None = None\n",
    "    type: QuestionType\n",
    "    correlationToken: str | None = None\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: str\n",
    "    correlationToken: str | None = None\n",
    "    promptTokensUsed: int | None = None\n",
    "    completionTokensUsed: int | None = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Mission: \n",
    "Adjust the function below. Ensure the answers provided are correct and in a short precise format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def ask_question(ask: Ask):\n",
    "    # \"\"\"\n",
    "    # # Ask a question\n",
    "    # \"\"\"\n",
    "\n",
    "    # Send a completion call to generate an answer\n",
    "    print('Sending a request to openai')\n",
    "    \n",
    "    start_phrase =  ask.question\n",
    "    response: openai.types.chat.chat_completion.ChatCompletion = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [{\"role\" : \"assistant\", \"content\" : start_phrase}, \n",
    "                     { \"role\" : \"system\", \"content\" : \"Answer this question:\"}]\n",
    "    )\n",
    "\n",
    "    print(response.choices[0].message.content)\n",
    "    print(response)\n",
    "\n",
    "    answer = Answer(answer=response.choices[0].message.content)\n",
    "    answer.correlationToken = ask.correlationToken\n",
    "    answer.promptTokensUsed = response.usage.prompt_tokens\n",
    "    answer.completionTokensUsed = response.usage.completion_tokens\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this snippet to try your method with several questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For this kind of question, the model should only answer with the letter of the correct answer\n",
    "ask = Ask(question=\"Which of the following is a color? a. Tree b. Flower c. Green\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about other types of questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this kind of question, the model should only answer with \"True\" or \"False\"\n",
    "ask = Ask(question=\"Is the sky blue?\", type=QuestionType.true_or_false)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this kind of question, the model should only answer with a number\n",
    "ask = Ask(question=\"How many stars are there in the universe?\", type=QuestionType.estimation)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this notebook, we have explored how to use OpenAI's language models to generate responses to various types of questions. We have set up the necessary environment, loaded environment variables, and initialized the Azure OpenAI client. We have also defined a function to generate answers to questions and tested it with various examples. This demonstrates the power of LLMs in providing accurate and contextually relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Up Next\n",
    "In the next lab we'll begin looking at AI orchestrators. Whereas the OpenAI library simplifies working with the OpenAI API, orchestrators take things to the next level!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
