{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d1b588",
   "metadata": {},
   "source": [
    "# Semantic Kernel Orchestrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "chat_completion_service = AzureChatCompletion(\n",
    "            service_id=\"azure_openai\",\n",
    "            deployment_name=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "            api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "            endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3999aaa5",
   "metadata": {},
   "source": [
    "You can start using the completion service right away or add the chat completion service to a kernel. You can use the following code to add a service to the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b11cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add the chat completion service created above to the kernel\n",
    "kernel.add_service(chat_completion_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06f44b",
   "metadata": {},
   "source": [
    "Once you've added chat completion services to your kernel, you can retrieve them using the get service method. Below is an example of how you can retrieve a chat completion service from the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "\n",
    "# Retrieve the chat completion service by type\n",
    "chat_completion_service = kernel.get_service(type=ChatCompletionClientBase)\n",
    "\n",
    "# Retrieve the chat completion service by id\n",
    "chat_completion_service = kernel.get_service(service_id=\"my-service-id\")\n",
    "\n",
    "# Retrieve the default inference settings\n",
    "execution_settings = kernel.get_prompt_execution_settings_from_service_id(\"my-service-id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e84e7cf",
   "metadata": {},
   "source": [
    "## Using chat completion service\n",
    "\n",
    "Now that you have a chat completion service, you can use it to generate responses from an AI agent. There are two main ways to use a chat completion service:\n",
    "\n",
    "**Non-streaming**: You wait for the service to generate an entire response before returning it to the user.\n",
    "\n",
    "**Streaming**: Individual chunks of the response are generated and returned to the user as they are created.\n",
    "Before getting started, you will need to manually create an execution settings instance to use the chat completion service if you did not register the service with the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e54ec6",
   "metadata": {},
   "source": [
    "Non-Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatHistory()\n",
    "chat_history.add_user_message(\"Hello, how are you?\")\n",
    "\n",
    "response = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fdaf6",
   "metadata": {},
   "source": [
    "Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d05422",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatHistory()\n",
    "chat_history.add_user_message(\"Hello, how are you?\")\n",
    "\n",
    "response = chat_completion.get_streaming_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    ")\n",
    "\n",
    "async for chunk in response:\n",
    "    print(chunk, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
