{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecff1399",
   "metadata": {},
   "source": [
    "# Function Calling\n",
    "\n",
    "The most powerful feature of chat completion is the ability to call functions from the model. This allows you to create a chat bot that can interact with your existing code, making it possible to automate business processes, create code snippets, and more.\n",
    "With Semantic Kernel, we simplify the process of using function calling by automatically describing your functions and their parameters to the model and then handling the back-and-forth communication between the model and your code.\n",
    "When using function calling, however, it's good to understand what's actually happening behind the scenes so that you can optimize your code and make the most of this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2f8d9",
   "metadata": {},
   "source": [
    "## How auto function calling works\n",
    "\n",
    "When you call a function from the model, the model generates a JSON object that describes the function and its parameters. This JSON object is then passed to your code, which can then execute the function with the provided parameters.\n",
    "The model is able to generate this JSON object because it has been trained on a large dataset of code and natural language, allowing it to understand the structure of functions and their parameters.\n",
    "The model also has a built-in understanding of the types of parameters that functions can take, which allows it to generate the correct JSON object for the function being called.\n",
    "The model uses a combination of natural language processing and code generation techniques to create the JSON object, which is then passed to your code for execution.\n",
    "\n",
    "When you make a request to a model with function calling enabled, Semantic Kernel performs the following steps:\n",
    "\n",
    "![FunctionCallingProcessSk](../../assets/images/functioncalling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f72df",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment. We'll use Semantic Kernel's Azure OpenAI connector to work with the OpenAI models that support function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb87f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureChatPromptExecutionSettings\n",
    ")\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.functions import kernel_function, KernelArguments\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "chat_completion_service = AzureChatCompletion(\n",
    "            service_id=\"azure_openai\",\n",
    "            deployment_name=os.environ.get(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\"),\n",
    "            api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "            endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a8aa7",
   "metadata": {},
   "source": [
    "At a high-level, a plugin is a group of functions that can be exposed to AI apps and services. The functions within plugins can then be orchestrated by an AI application to accomplish user requests. Within Semantic Kernel, you can invoke these functions automatically with function calling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd2d59",
   "metadata": {},
   "source": [
    "## Creating a Plugin with Functions\n",
    "\n",
    "Let's create a Weather plugin that includes two functions: one to get current weather data and another to get a weather forecast. In a real implementation, these functions would call external APIs, but for demonstration purposes, we'll return mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bd3e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a plugin with functions to get weather data and forecast\n",
    "class WeatherPlugin:\n",
    "    \"\"\"\n",
    "    A plugin to get weather data and forecast\n",
    "    \"\"\"\n",
    "    # Define functions for function calling\n",
    "    @kernel_function(name=\"get_weather_data\", description=\"Get weather data for a specific location\")\n",
    "    async def get_weather_data(self, location: str) -> str:\n",
    "        \"\"\"\n",
    "        Stub function to get weather data for a specific location\n",
    "        In a real implementation, this would call a weather API\n",
    "        \"\"\"\n",
    "        print(f\"Getting weather data for {location}\")\n",
    "        weather_data = {\n",
    "            \"temperature\": 25,  # Celsius\n",
    "            \"cloud_cover\": 0.2,  # 20%\n",
    "            \"irradiance\": 800,  # W/m²\n",
    "            \"precipitation_chance\": 0.1,  # 10%\n",
    "        }\n",
    "        return json.dumps(weather_data)\n",
    "\n",
    "    @kernel_function(name=\"get_weather_forecast\", description=\"Get weather forecast for a specific location for the next day\")\n",
    "    async def get_weather_forecast(self, location: str) -> str:\n",
    "        \"\"\"\n",
    "        Stub function to get weather forecast for a specific location\n",
    "        In a real implementation, this would call a weather API\n",
    "        \"\"\"\n",
    "        print(f\"Getting weather forecast for {location}\")\n",
    "        forecast_data = {\n",
    "            \"temperature\": 28,  # Celsius\n",
    "            \"cloud_cover\": 0.1,  # 10%\n",
    "            \"irradiance\": 900,  # W/m²\n",
    "            \"precipitation_chance\": 0.05,  # 5%\n",
    "        }\n",
    "        return json.dumps(forecast_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ddf542",
   "metadata": {},
   "source": [
    "## Setting Up the Kernel\n",
    "\n",
    "Now, let's set up the Semantic Kernel and add our chat completion service and weather plugin. The kernel serves as the central orchestrator for our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1dc6e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='weather_plugin', description=None, functions={'get_weather_data': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_weather_data', plugin_name='weather_plugin', description='Get weather data for a specific location', parameters=[KernelParameterMetadata(name='location', description=None, default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fffe34e91c0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fffad3d2fc0>, method=<bound method WeatherPlugin.get_weather_data of <__main__.WeatherPlugin object at 0x7fffad46d4f0>>, stream_method=None), 'get_weather_forecast': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_weather_forecast', plugin_name='weather_plugin', description='Get weather forecast for a specific location for the next day', parameters=[KernelParameterMetadata(name='location', description=None, default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fffad46c9e0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fffad46d580>, method=<bound method WeatherPlugin.get_weather_forecast of <__main__.WeatherPlugin object at 0x7fffad46d4f0>>, stream_method=None)})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add the chat completion service created above to the kernel\n",
    "kernel.add_service(chat_completion_service)\n",
    "\n",
    "# Add the weather plugin to the kernel\n",
    "# The plugin is a group of functions that can be exposed to AI apps and services\n",
    "kernel.add_plugin(WeatherPlugin(), plugin_name=\"weather_plugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d77918",
   "metadata": {},
   "source": [
    "## Testing Function Calling\n",
    "\n",
    "Now let's test our function calling capabilities. We'll create a chat history, add a user message asking about the weather, and then use our chat completion service with auto function calling enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9aaf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather data for Seattle\n",
      "Assistant: The weather in Seattle is currently 25°C with 20% cloud cover and an irradiance level of 800. There is a 10% chance of precipitation.\n",
      "Getting weather forecast for Seattle\n",
      "Assistant: The weather forecast for Seattle tomorrow is a temperature of 28°C with 10% cloud cover and an irradiance level of 900. There is a 5% chance of precipitation.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the chat history\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "user_input = \"What is the weather like in Seattle?\"\n",
    "system_input = \"You are a helpful assistant that provides weather information. Keep it short and concise.\"\n",
    "chat_history.add_user_message(user_input)\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "response = await chat_completion_service.get_chat_message_content(\n",
    "    chat_history=chat_history,\n",
    "    kernel=kernel,\n",
    "    settings=execution_settings,\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {response}\")\n",
    "\n",
    "# Add the assistant's response to the chat history\n",
    "chat_history.add_assistant_message(response.content)\n",
    "\n",
    "chat_history.add_user_message(\"What is the weather like in Seattle tomorrow?\")\n",
    "response = await chat_completion_service.get_chat_message_content(\n",
    "    chat_history=chat_history,\n",
    "    kernel=kernel,\n",
    "    settings=execution_settings,\n",
    ")\n",
    "print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6bbf36",
   "metadata": {},
   "source": [
    "## Creating More Complex Functions\n",
    "\n",
    "Now let's create some additional functions that are more complex and demonstrate how they can be used together in a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "318bb6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='weather_analysis_plugin', description=None, functions={'analyze_weather_impact': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='analyze_weather_impact', plugin_name='weather_analysis_plugin', description='Analyze how weather will impact outdoor activities', parameters=[KernelParameterMetadata(name='weather_data', description=None, default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), KernelParameterMetadata(name='activity_type', description=None, default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fffacb5d2e0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fffacb5d220>, method=<bound method WeatherAnalysisPlugin.analyze_weather_impact of <__main__.WeatherAnalysisPlugin object at 0x7fffad3d2420>>, stream_method=None)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WeatherAnalysisPlugin:\n",
    "    \"\"\"\n",
    "    A plugin to analyze weather data and provide recommendations\n",
    "    \"\"\"\n",
    "    @kernel_function(name=\"analyze_weather_impact\", description=\"Analyze how weather will impact outdoor activities\")\n",
    "    async def analyze_weather_impact(self, weather_data: str, activity_type: str) -> str:\n",
    "        \"\"\"\n",
    "        Analyze weather data and provide recommendations for outdoor activities\n",
    "        Args:\n",
    "            weather_data: JSON string containing weather information\n",
    "            activity_type: Type of activity (hiking, picnic, sports, etc.)\n",
    "        \"\"\"\n",
    "        print(f\"Analyzing weather impact for {activity_type}\")\n",
    "        # In a real implementation, this would analyze the weather data and return recommendations\n",
    "        weather = json.loads(weather_data)\n",
    "        \n",
    "        analysis = {\n",
    "            \"activity\": activity_type,\n",
    "            \"suitable\": weather[\"temperature\"] > 20 and weather[\"precipitation_chance\"] < 0.3,\n",
    "            \"recommendations\": [],\n",
    "        }\n",
    "        \n",
    "        if weather[\"temperature\"] > 30:\n",
    "            analysis[\"recommendations\"].append(\"Bring plenty of water due to high temperature\")\n",
    "        if weather[\"cloud_cover\"] < 0.3:\n",
    "            analysis[\"recommendations\"].append(\"Bring sunscreen as UV exposure will be high\")\n",
    "        if weather[\"precipitation_chance\"] > 0.1:\n",
    "            analysis[\"recommendations\"].append(\"Consider bringing rain gear just in case\")\n",
    "            \n",
    "        return json.dumps(analysis)\n",
    "\n",
    "# Add the new plugin to the kernel\n",
    "kernel.add_plugin(WeatherAnalysisPlugin(), plugin_name=\"weather_analysis_plugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a7e88",
   "metadata": {},
   "source": [
    "## Testing Complex Function Interactions\n",
    "\n",
    "Let's test how the model can chain multiple function calls together to respond to a more complex question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37bff03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather forecast for Seattle\n",
      "Analyzing weather impact for hiking\n",
      "Assistant: The weather in Seattle tomorrow is forecasted to be suitable for hiking. \n",
      "\n",
      "**Recommendations:**\n",
      "- **Bring sunscreen** as UV exposure will be high. \n",
      "\n",
      "Enjoy your hiking trip!\n"
     ]
    }
   ],
   "source": [
    "# Create a new chat history for complex interaction\n",
    "complex_chat = ChatHistory()\n",
    "\n",
    "# Ask a more complex question that requires multiple function calls\n",
    "complex_chat.add_user_message(\"I'm planning a hiking trip in Seattle tomorrow. Is the weather going to be good for hiking?\")\n",
    "\n",
    "# Use the same execution settings with auto function calling\n",
    "complex_response = await chat_completion_service.get_chat_message_content(\n",
    "    chat_history=complex_chat,\n",
    "    kernel=kernel,\n",
    "    settings=execution_settings,\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {complex_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ccd6e7",
   "metadata": {},
   "source": [
    "## Understanding Function Calling in the Background\n",
    "\n",
    "Let's examine what's happening in the background during function calling. We'll set up a special execution setting that lets us see the actual function calls being made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2672444e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather data for Paris\n",
      "Getting weather forecast for Paris\n",
      "Analyzing weather impact for picnic\n",
      "Function calling metadata:\n",
      "{'logprobs': None, 'id': 'chatcmpl-BF5WWTZc7z4LqHoFg4SYBAMKLOAdq', 'created': 1742935852, 'system_fingerprint': 'fp_ded0d14823', 'usage': CompletionUsage(prompt_tokens=358, completion_tokens=62)}\n",
      "\n",
      "Final response: The current temperature in Paris is 25°C.\n",
      "\n",
      "For your picnic tomorrow, the weather forecast predicts a temperature of 28°C with minimal cloud cover and very low chance of precipitation. It will be suitable for a picnic. Just remember to bring sunscreen as UV exposure will be high. Enjoy your day!\n"
     ]
    }
   ],
   "source": [
    "# Create a chat history to observe function calls\n",
    "observation_chat = ChatHistory()\n",
    "observation_chat.add_user_message(\"What's the temperature in Paris right now, and will it be good for a picnic tomorrow?\")\n",
    "\n",
    "# Create execution settings with debug info\n",
    "debug_settings = AzureChatPromptExecutionSettings()\n",
    "debug_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "debug_settings.extension_data = {\"include_raw_responses\": True}  # This depends on the specific implementation\n",
    "\n",
    "# Get response with debug information\n",
    "debug_response = await chat_completion_service.get_chat_message_content(\n",
    "    chat_history=observation_chat,\n",
    "    kernel=kernel,\n",
    "    settings=debug_settings,\n",
    ")\n",
    "\n",
    "# Print the debug information if available\n",
    "if hasattr(debug_response, 'metadata') and debug_response.metadata:\n",
    "    print(\"Function calling metadata:\")\n",
    "    print(debug_response.metadata)\n",
    "    \n",
    "print(f\"\\nFinal response: {debug_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555b4f3",
   "metadata": {},
   "source": [
    "## Creating a Custom Function Choice Behavior\n",
    "\n",
    "In some cases, you might want more control over which functions are called and how. Let's create a custom function choice behavior that allows for more control over the function calling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ef27e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for FunctionChoiceBehavior\nfilters.included_functions:.[key]\n  Input should be 'excluded_plugins', 'included_plugins', 'excluded_functions' or 'included_functions' [type=literal_error, input_value='included_functions:', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/literal_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Create execution settings with custom handler\u001b[39;00m\n\u001b[32m     18\u001b[39m custom_settings = AzureChatPromptExecutionSettings()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m custom_settings.function_choice_behavior = \u001b[43mFunctionChoiceBehavior\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRequired\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mincluded_functions:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustom_function_handler\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Create a chat with a question that will trigger function calling\u001b[39;00m\n\u001b[32m     22\u001b[39m custom_chat = ChatHistory()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/semantic_kernel/connectors/ai/function_choice_behavior.py:174\u001b[39m, in \u001b[36mFunctionChoiceBehavior.Required\u001b[39m\u001b[34m(cls, auto_invoke, filters, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates a FunctionChoiceBehavior with type REQUIRED.\u001b[39;00m\n\u001b[32m    168\u001b[39m \n\u001b[32m    169\u001b[39m \u001b[33;03mReturns FunctionChoiceBehavior class with auto_invoke enabled, and the desired functions\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[33;03mbased on either the specified filters or the full qualified names. The model is required to use one of the\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[33;03mprovided functions to complete a given task/query.\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    173\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mmaximum_auto_invoke_attempts\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_invoke \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFunctionChoiceType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREQUIRED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic/main.py:214\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    213\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    216\u001b[39m     warnings.warn(\n\u001b[32m    217\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    218\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    219\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    220\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    221\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for FunctionChoiceBehavior\nfilters.included_functions:.[key]\n  Input should be 'excluded_plugins', 'included_plugins', 'excluded_functions' or 'included_functions' [type=literal_error, input_value='included_functions:', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/literal_error"
     ]
    }
   ],
   "source": [
    "# Create a custom function choice behavior\n",
    "class CustomPlugin:\n",
    "    \"\"\"\n",
    "    A custom plugin to handle function calls\n",
    "    \"\"\"\n",
    "    @kernel_function(name=\"custom_function_handler\", description=\"Custom function handler\")\n",
    "    async def custom_function_handler(self, functions_to_call: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Custom function handler to modify or filter function calls\n",
    "        \"\"\"\n",
    "        print(f\"Custom function handler called with: {functions_to_call}\")\n",
    "        # Modify the functions to call if needed\n",
    "        return functions_to_call\n",
    "\n",
    "kernel.add_plugin(CustomPlugin(), plugin_name=\"custom_plugin\")\n",
    "\n",
    "# Create execution settings with custom handler\n",
    "custom_settings = AzureChatPromptExecutionSettings()\n",
    "custom_settings.function_choice_behavior = FunctionChoiceBehavior.Required(filters={\"included_functions\": [\"custom_function_handler\"]})\n",
    "\n",
    "# Create a chat with a question that will trigger function calling\n",
    "custom_chat = ChatHistory()\n",
    "custom_chat.add_user_message(\"What's the weather in Tokyo and will it rain tomorrow?\")\n",
    "\n",
    "# Get response with custom function handling\n",
    "custom_response = await chat_completion_service.get_chat_message_content(\n",
    "    chat_history=custom_chat,\n",
    "    kernel=kernel,\n",
    "    settings=custom_settings,\n",
    ")\n",
    "\n",
    "print(f\"\\nCustom handler response: {custom_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0103a79",
   "metadata": {},
   "source": [
    "## Exercise: Creating Your Own Plugin and Testing Function Calling\n",
    "\n",
    "Now it's your turn! Create a new plugin that provides useful functions for a specific domain of your choice. Then test how the model uses these functions in response to user queries.\n",
    "\n",
    "Here's a template to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own plugin\n",
    "class MyCustomPlugin:\n",
    "    \"\"\"\n",
    "    Description of your custom plugin\n",
    "    \"\"\"\n",
    "    @kernel_function(name=\"my_function1\", description=\"Description of what this function does\")\n",
    "    async def my_function1(self, param1: str, param2: str = \"default\") -> str:\n",
    "        \"\"\"\n",
    "        Detailed description of what this function does\n",
    "        \n",
    "        Args:\n",
    "            param1: Description of param1\n",
    "            param2: Description of param2\n",
    "        \"\"\"\n",
    "        # Your function implementation here\n",
    "        print(f\"Function called with {param1} and {param2}\")\n",
    "        return json.dumps({\"result\": f\"Processed {param1} with {param2}\"})\n",
    "    \n",
    "    @kernel_function(name=\"my_function2\", description=\"Description of what this function does\")\n",
    "    async def my_function2(self, input_data: str) -> str:\n",
    "        \"\"\"Another function implementation\"\"\"\n",
    "        # Your function implementation here\n",
    "        return f\"Result: {input_data}\"\n",
    "\n",
    "# Add your plugin to the kernel\n",
    "# kernel.add_plugin(MyCustomPlugin(), plugin_name=\"my_custom_plugin\")\n",
    "\n",
    "# Test your plugin with a user query\n",
    "# my_chat = ChatHistory()\n",
    "# my_chat.add_user_message(\"A query that would trigger your custom functions\")\n",
    "# \n",
    "# my_response = await chat_completion_service.get_chat_message_content(\n",
    "#     chat_history=my_chat,\n",
    "#     kernel=kernel,\n",
    "#     settings=execution_settings,\n",
    "# )\n",
    "# \n",
    "# print(f\"Response: {my_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945b926",
   "metadata": {},
   "source": [
    "## Advanced Challenge: Multi-Function Conversation Agent\n",
    "\n",
    "Now that you understand how function calling works, here's a more advanced challenge:\n",
    "\n",
    "Create a full-featured conversational agent that can:\n",
    "\n",
    "1. Use multiple functions from different plugins to answer complex queries\n",
    "2. Maintain conversation context across multiple turns\n",
    "3. Handle errors gracefully if a function call fails\n",
    "4. Provide helpful responses that synthesize information from multiple function calls\n",
    "\n",
    "Example implementation areas:\n",
    "- A travel assistant that checks weather, flight status, and hotel availability\n",
    "- A personal finance assistant that can check balances, analyze spending, and suggest budgets\n",
    "- A health and wellness assistant that tracks activity, suggests workouts, and monitors nutrition\n",
    "\n",
    "Bonus points if you implement:\n",
    "- Function call tracing/logging\n",
    "- User authentication for sensitive functions\n",
    "- Fallback mechanisms when functions aren't available\n",
    "\n",
    "Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
